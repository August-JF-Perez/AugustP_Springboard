{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "EvpyI0E7SpNT"
   },
   "source": [
    "## Random Forest\n",
    "\n",
    "Random Forest is an ensemble of Decision Trees. With a few exceptions, a `RandomForestClassifier` has all the hyperparameters of a `DecisionTreeClassifier` (to control how trees are grown), plus all the hyperparameters of a `BaggingClassifier` to control the ensemble itself.\n",
    "\n",
    "The Random Forest algorithm introduces extra randomness when growing trees; instead of searching for the very best feature when splitting a node, it searches for the best feature among a random subset of features. This results in a greater tree diversity, which (once again) trades a higher bias for a lower variance, generally yielding an overall better model. The following `BaggingClassifier` is roughly equivalent to the previous `RandomForestClassifier`. Run the cell below to visualize a single estimator from a random forest model, using the Iris dataset to classify the data into the appropriate species."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000,
     "output_embedded_package_id": "1NIbktS4yyfVlE2Y4bXMargRbQgbdWTFh"
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 7579,
     "status": "ok",
     "timestamp": 1592213046926,
     "user": {
      "displayName": "Andrew Maguire",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14GjaIpd1sqQPWOc9NJXtyl5fYSonikxEZgshlvloAYk=s64",
      "userId": "13447906511017779027"
     },
     "user_tz": -60
    },
    "id": "z_-6xEUFSpNU",
    "outputId": "75184be3-e99c-4c44-a638-824a9ba0b1e9",
    "tags": []
   },
   "outputs": [],
   "source": [
    "#AP commenting out exporting code with two ## so export files aren't updated unneccessarily every time the cell is run\n",
    "from sklearn.datasets import load_iris\n",
    "##iris = load_iris()\n",
    "\n",
    "# Model (can also use single decision tree)\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "##model = RandomForestClassifier(n_estimators=10)\n",
    "\n",
    "# Train\n",
    "##model.fit(iris.data, iris.target)\n",
    "# Extract single tree\n",
    "##estimator = model.estimators_[5]\n",
    "\n",
    "from sklearn.tree import export_graphviz\n",
    "# Export as dot file\n",
    "##export_graphviz(estimator, out_file='tree.dot', \n",
    "##                feature_names = iris.feature_names,\n",
    "##                class_names = iris.target_names,\n",
    "##                rounded = True, proportion = False, \n",
    "##                precision = 2, filled = True)\n",
    "\n",
    "# Convert to png using system command (requires Graphviz)\n",
    "from subprocess import call\n",
    "##call(['dot', '-Tpng', 'tree.dot', '-o', 'tree.png', '-Gdpi=600'])\n",
    "\n",
    "# Display in jupyter notebook\n",
    "from IPython.display import Image\n",
    "##Image(filename = 'tree.png')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "VSnWoCRUSpNY"
   },
   "source": [
    "Notice how each split seperates the data into buckets of similar observations. This is a single tree and a relatively simple classification dataset, but the same method is used in a more complex dataset with greater depth to the trees."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "tJ2aGPMZB5X8"
   },
   "source": [
    "## Coronavirus\n",
    "Coronavirus disease (COVID-19) is an infectious disease caused by a new virus.\n",
    "The disease causes respiratory illness (like the flu) with symptoms such as a cough, fever, and in more severe cases, difficulty breathing. You can protect yourself by washing your hands frequently, avoiding touching your face, and avoiding close contact (1 meter or 3 feet) with people who are unwell. An outbreak of COVID-19 started in December 2019 and at the time of the creation of this project was continuing to spread throughout the world. Many governments recommended only essential outings to public places and closed most business that do not serve food or sell essential items. An excellent [spatial dashboard](https://www.arcgis.com/apps/opsdashboard/index.html#/bda7594740fd40299423467b48e9ecf6) built by Johns Hopkins shows the daily confirmed cases by country. \n",
    "\n",
    "This case study was designed to drive home the important role that data science plays in real-world situations like this pandemic. This case study uses the Random Forest Classifier and a dataset from the South Korean cases of COVID-19 provided on [Kaggle](https://www.kaggle.com/kimjihoo/coronavirusdataset) to encourage research on this important topic. The goal of the case study is to build a Random Forest Classifier to predict the 'state' of the patient."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "7PrMkSjBQEMZ"
   },
   "source": [
    "First, please load the needed packages and modules into Python. Next, load the data into a pandas dataframe for ease of use."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "X3EhD-LSB5YI"
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "from datetime import datetime,timedelta\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "%matplotlib inline\n",
    "import plotly.graph_objects as go\n",
    "from sklearn.experimental import enable_iterative_imputer\n",
    "from sklearn.impute import IterativeImputer\n",
    "from sklearn.ensemble import ExtraTreesRegressor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "CiJQlTK1SpNd"
   },
   "outputs": [],
   "source": [
    "url ='SouthKoreacoronavirusdataset/PatientInfo.csv'\n",
    "df = pd.read_csv(url)\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "AP: Notes about the cols from the dataset source:\n",
    "\n",
    "- infection_case\n",
    "    - The value 'overseas inflow' means that the infection is from other country.\n",
    "    - The value 'etc' includes individual cases, cases where relevance classification is ongoing after investigation, and cases under investigation.\n",
    "- city: City(-si) / Country (-gun) / District (-gu)\n",
    "    - The value 'from other city' means that where the group infection started is other city.\n",
    "- age: the age of the patient\n",
    "    - 0s: 0 ~ 9\n",
    "    - 10s: 10 ~ 19\n",
    "    - 90s: 90 ~ 99\n",
    "    - 100s: 100 ~ 109\n",
    "- patient_id: the ID of the patient\n",
    "    - patient_id(10) = region_code(5) + patient_number(5)\n",
    "    - You can check the region_code in 'Region.csv' #AP: Don't have this datafile\n",
    "    - There are two types of the patient_number\n",
    "        - 1) local_num: The number given by the local government.\n",
    "        - 2) global_num: The number given by the KCDC\n",
    "- sex: the sex of the patient\n",
    "- age: the age of the patient\n",
    "    - 0s: 0 ~ 9\n",
    "    - 10s: 10 ~ 19\n",
    "    - 90s: 90 ~ 99\n",
    "    - 100s: 100 ~ 109\n",
    "- country: the country of the patient\n",
    "- province: the province of the patient\n",
    "- city: the city of the patient\n",
    "- infection_case: the case of infection\n",
    "- infected_by: the ID of who infected the patient\n",
    "    - This column refers to the 'patient_id' column.\n",
    "- contact_number: the number of contacts with people\n",
    "- symptom_onset_date: the date of symptom onset\n",
    "- confirmed_date: the date of being confirmed\n",
    "- released_date: the date of being released\n",
    "- deceased_date: the date of being deceased\n",
    "- state: isolated / released / deceased\n",
    "    - isolated: being isolated in the hospital\n",
    "    - released: being released from the hospital\n",
    "    - deceased: being deceased"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "GUY5Cp2cSpNg"
   },
   "outputs": [],
   "source": [
    "df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "oxKUKR_pSpNi"
   },
   "outputs": [],
   "source": [
    "#Counts of null values \n",
    "na_df=pd.DataFrame(df.isnull().sum().sort_values(ascending=False)).reset_index()\n",
    "na_df.columns = ['VarName', 'NullCount']\n",
    "na_df[(na_df['NullCount']>0)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "NhPtmRWdSpNl"
   },
   "outputs": [],
   "source": [
    "#counts of response variable values\n",
    "df.state.value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "06e0gqMzSpNp"
   },
   "source": [
    " **<font color='teal'> Create a new column named 'n_age' which is the calculated age based on the birth year column.</font>**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "DVCW7LBRSpNp"
   },
   "outputs": [],
   "source": [
    "df['birth_year'].dtypes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "df['n_age'] = datetime.now().year - df['birth_year']\n",
    "df['n_age'].head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "w9t91IzDSpNr"
   },
   "source": [
    "### Handle Missing Values"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "Fm1TjGDhSpNs"
   },
   "source": [
    " **<font color='teal'> Print the number of missing values by column.</font>**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "PAtr2t3rSpNs"
   },
   "outputs": [],
   "source": [
    "print(df.isnull().sum().sort_values(ascending=False))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "I8B5cY19SpNu"
   },
   "outputs": [],
   "source": [
    "df.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "zjLpYG_ASpNw"
   },
   "source": [
    " **<font color='teal'> Fill the 'disease' missing values with 0 and remap the True values to 1.</font>**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "df['disease'].unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "NHBtVW1ESpNx"
   },
   "outputs": [],
   "source": [
    "df[\"disease\"] = df[\"disease\"].fillna(0)\n",
    "df[\"disease\"] = df[\"disease\"].replace('True', 1)\n",
    "df['disease'].unique()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "fZArBBUhSpNz"
   },
   "source": [
    " **<font color='teal'> Fill null values in the following columns with their mean: 'global_number','birth_year','infection_order','infected_by'and 'contact_number'</font>**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "zQdarDx_SpNz"
   },
   "outputs": [],
   "source": [
    "df[\"global_num\"] = df[\"global_num\"].fillna(df['global_num'].mean())\n",
    "#AP I think either a ffill or bfill method would be more appropriate since this seems to be a running total of COVID cases\n",
    "df[\"birth_year\"] = df[\"birth_year\"].fillna(df['birth_year'].mean())\n",
    "df[\"infection_order\"] = df[\"infection_order\"].fillna(df['infection_order'].mean())\n",
    "#AP: note: I don't agree with using mean since this seems to denote order of infection from one person to another, it's categorical in nature. Using so I can continue with case study and maybe figure out why this choice was made\n",
    "df[\"infected_by\"] = df[\"infected_by\"].fillna(df['infected_by'].mean())\n",
    "#AP: note: I don't agree with using mean since this notes the paitent ID that infected the person. Using so I can continue with case study and maybe figure out why this choice was made\n",
    "df[\"contact_number\"] = df[\"contact_number\"].fillna(df['contact_number'].mean())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "Coy_jYEbSpN2"
   },
   "source": [
    " **<font color='teal'> Fill the rest of the missing values with any method.</font>**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# AP HERE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "ATJ84cdDSpN2"
   },
   "outputs": [],
   "source": [
    "df.isnull().sum().sort_values(ascending=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "#AP: To get an idea of the data in each col\n",
    "null_remaining_cols = ['deceased_date', 'symptom_onset_date', 'released_date', 'infection_case', 'n_age', 'age', 'sex', 'confirmed_date', 'state', 'city']\n",
    "df[null_remaining_cols].head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df[null_remaining_cols].dtypes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "AP: Plan for dealing with each col's missing values:\n",
    "\n",
    "- deceased_date: -DROPPING ANYWAY- Figure out what to impute to denote patient not deceased, zero (0)(?)\n",
    "- symptom_onset_date: -DROPPING ANYWAY- Figure out what to impute if no symptoms, zero (0) to possibly allow calc's with date style col(?)\n",
    "- released_date: -DONE- -DROPPING ANYWAY- 'unknown' since nulls vastly outnumber actual values\n",
    "- infection_case: -DONE- impute and replace 'etc' with 'unknown' ~fill with \"contact with patient\" (would this make the data too skewed towards that infection vector?)~\n",
    "- n_age: -DONE- mean\n",
    "- age: -DONE- mean (rounded to nearest tens place)\n",
    "- sex: -DONE- fill half with male & half with female\n",
    "    - could do df['sex'].ffill() but that seems to make it too lopsided toward female\n",
    "    - combine df['sex'].sample(frac=1, random_state=1234) with .ffill()\n",
    "- confirmed_date: -DONE- -DROPPING ANYWAY- mean\n",
    "- state: -DONE- mode\n",
    "- city: -DONE- mode"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# confirmed_date col, impute with mean\n",
    "# this col getting dropped later but keeping this code as good example\n",
    "df['confirmed_date'] = pd.to_datetime(df['confirmed_date'].str.strip(), format='%Y-%m-%d')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [],
   "source": [
    "df['confirmed_date'].value_counts(dropna=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "datetime.date(df['confirmed_date'].mean())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# this col getting dropped later but keeping this code as good example\n",
    "df['confirmed_date'] = df['confirmed_date'].fillna(datetime.date(df['confirmed_date'].mean()))\n",
    "df['confirmed_date'].value_counts(dropna=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# n_age col, impute with mean (rounded to nearest year)\n",
    "df['n_age'] = df['n_age'].fillna(round(df['n_age'].mean(),0))\n",
    "df['n_age'].value_counts(dropna=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# age col, impute with mean rounded to tens place\n",
    "\n",
    "# first need to drop the 's'\n",
    "df['age'] = df['age'].str.replace('s', '')\n",
    "df['age'] = df['age'].astype('float64')\n",
    "df['age'].value_counts(dropna=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['age'] = df['age'].fillna(round(df['age'].mean(), -1))\n",
    "#rename 'age' col to more appropriate 'age_group'\n",
    "df = df.rename(columns={'age': 'age_group'})\n",
    "df['age_group'].value_counts(dropna=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Infection Case col: impute with 'unknown'\n",
    "df['infection_case'] = df['infection_case'].fillna('unknown')\n",
    "\n",
    "# also change etc value to 'unknown' since functionally the same but 'unknown' is more readily understood\n",
    "df['infection_case'] = df['infection_case'].replace('etc', 'unknown')\n",
    "\n",
    "df['infection_case'].value_counts(dropna=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# City & state cols: impute with mode for each\n",
    "city_state_fill = {'city' : df['city'].mode()[0], 'state' : df['state'].mode()[0]}\n",
    "df[['city', 'state']] = df[['city', 'state']].fillna(value=city_state_fill)\n",
    "#AP df['city'].mode() outputs a dict-like object, need to specify the value for it to work\n",
    "#df['city'] = df['city'].replace(to_replace = np.nan, value = df['city'].mode()[0])\n",
    "    #potentially useful code to do same thing, didn't test since I figured out other code's problem\n",
    "print(df[['city', 'state']].isnull().sum().sort_values(ascending=False))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# released_date col: impute with 'unknown'\n",
    "# this col getting dropped later but keeping this code as good example\n",
    "df['released_date'].value_counts(dropna=False)\n",
    "df['released_date'] = df['released_date'].fillna('unkown')\n",
    "print(df[['released_date']].isnull().sum().sort_values(ascending=False))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# sex col: fill half with 'male' & other half with 'female'. Attempting to do so randomly\n",
    "sex_missing_cols, sex_missing_rows = (\n",
    "    (df[['sex']].isnull().sum(x) | df[['sex']].eq('').sum(x))\n",
    "    .loc[lambda x: x.gt(0)].index\n",
    "    for x in (0, 1))\n",
    "print(df['sex'].value_counts(dropna=False))\n",
    "df[['sex']].loc[sex_missing_rows, sex_missing_cols]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#df[['sex']].loc[sex_missing_rows, sex_missing_cols] = df[['sex']].loc[sex_missing_rows, sex_missing_cols]\n",
    "df['sex'] = df['sex'].sample(frac=1, random_state=1234).ffill()\n",
    "#a = df.loc[df.sex == 'NaN', 'sex']\n",
    "#df.loc[(a if len(a.index) < 72 else a.sample(72)).index, 'sex'] = 'male'\n",
    "df['sex'].value_counts(dropna=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#AP: Handy piece of code I learned from: https://stackoverflow.com/questions/27159189/find-empty-or-nan-entry-in-pandas-dataframe\n",
    "missing_cols, missing_rows = (\n",
    "    (df[['city']].isnull().sum(x) | df[['city']].eq('').sum(x))\n",
    "    .loc[lambda x: x.gt(0)].index\n",
    "    for x in (0, 1))\n",
    "df[['city']].loc[missing_rows, missing_cols]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "kGi4NkcbSpN4"
   },
   "source": [
    " **<font color='teal'> Check for any remaining null values.</font>**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(df.isnull().sum().sort_values(ascending=False))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "PK9Jk8KgSpN9"
   },
   "source": [
    "Remove date columns from the data.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "xIbYDncMSpN9"
   },
   "outputs": [],
   "source": [
    "df = df.drop(['symptom_onset_date','confirmed_date','released_date','deceased_date'],axis =1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "RDBxf5ZDB5ZZ"
   },
   "source": [
    "Review the count of unique values by column."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "PIdCkZ4AB5Zf"
   },
   "outputs": [],
   "source": [
    "print(df.nunique())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "9oENi5DRB5Zq"
   },
   "source": [
    "Review the percent of unique values by column."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "_IcO33VsB5Zt"
   },
   "outputs": [],
   "source": [
    "print(df.nunique()/df.shape[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "rCZHVugqB5Z4"
   },
   "source": [
    "Review the range of values per column."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "Y3zLsGxMB5Z5"
   },
   "outputs": [],
   "source": [
    "#df.describe().T\n",
    "# AP: I like the non-transposed better\n",
    "df.describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "AEBFq3hmB5aN"
   },
   "source": [
    "### Check for duplicated rows"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "s9KGFCpkB5aP"
   },
   "outputs": [],
   "source": [
    "duplicateRowsDF = df[df.duplicated()]\n",
    "duplicateRowsDF"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "8WOrbEk1SpOH"
   },
   "source": [
    "Print the categorical columns and their associated levels."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "8QTm6RuRSpOH"
   },
   "outputs": [],
   "source": [
    "dfo = df.select_dtypes(include=['object'], exclude=['datetime'])\n",
    "dfo.shape\n",
    "#get levels for all variables\n",
    "vn = pd.DataFrame(dfo.nunique()).reset_index()\n",
    "vn.columns = ['VarName', 'LevelsCount']\n",
    "vn.sort_values(by='LevelsCount', ascending =False)\n",
    "vn"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "a43l6w_uSpOJ"
   },
   "source": [
    "**<font color='teal'> Plot the correlation heat map for the features.</font>**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.dtypes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "ZRJlPqV5B5e5"
   },
   "outputs": [],
   "source": [
    "plt.figure()\n",
    "sns.heatmap(df.corr(numeric_only=True), annot=True)\n",
    "#AP: numeric_only since default is false"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "2KtABW0USpOK"
   },
   "source": [
    "**<font color='teal'> Plot the boxplots to check for outliers. </font>**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "uYMmU_szB5fZ"
   },
   "outputs": [],
   "source": [
    "df.boxplot(by='state', figsize=(10,10))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "2Z_NuqkNSpOM"
   },
   "source": [
    "**<font color='teal'> Create dummy features for object type features. </font>**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "0uTSQY_liDHj"
   },
   "source": [
    "### Split the data into test and train subsamples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "BSkPut0gguds"
   },
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# dont forget to define your X and y\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=.2, random_state=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "TcrOg3y7gRtG"
   },
   "source": [
    "### Scale data to prep for model creation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "IZL-q-KtAYI6"
   },
   "outputs": [],
   "source": [
    "#scale data\n",
    "from sklearn import preprocessing\n",
    "import numpy as np\n",
    "# build scaler based on training data and apply it to test data to then also scale the test data\n",
    "scaler = preprocessing.StandardScaler().fit(X_train)\n",
    "X_train_scaled=scaler.transform(X_train)\n",
    "X_test_scaled=scaler.transform(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "ZSOICugNSpOR"
   },
   "outputs": [],
   "source": [
    "from sklearn.metrics import precision_recall_curve\n",
    "from sklearn.metrics import f1_score\n",
    "from sklearn.metrics import auc\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from matplotlib import pyplot\n",
    "from sklearn.metrics import precision_recall_curve\n",
    "from sklearn.metrics import f1_score\n",
    "from sklearn.metrics import auc\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import classification_report,confusion_matrix,roc_curve,roc_auc_score\n",
    "from sklearn.metrics import accuracy_score,log_loss\n",
    "from matplotlib import pyplot"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "vzrLoM58SpOT"
   },
   "source": [
    "### Fit Random Forest Classifier\n",
    "The fit model shows an overall accuracy of 80% which is great and indicates our model was effectively able to identify the status of a patients in the South Korea dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "b9jQyje3SpOU"
   },
   "outputs": [],
   "source": [
    "from sklearn.ensemble import RandomForestClassifier\n",
    "clf = RandomForestClassifier(n_estimators=300, random_state = 1,n_jobs=-1)\n",
    "model_res = clf.fit(X_train_scaled, y_train)\n",
    "y_pred = model_res.predict(X_test_scaled)\n",
    "y_pred_prob = model_res.predict_proba(X_test_scaled)\n",
    "lr_probs = y_pred_prob[:,1]\n",
    "ac = accuracy_score(y_test, y_pred)\n",
    "\n",
    "f1 = f1_score(y_test, y_pred, average='weighted')\n",
    "cm = confusion_matrix(y_test, y_pred)\n",
    "\n",
    "print('Random Forest: Accuracy=%.3f' % (ac))\n",
    "\n",
    "print('Random Forest: f1-score=%.3f' % (f1))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "QYdW02QQSpOW"
   },
   "source": [
    "### Create Confusion Matrix Plots\n",
    "Confusion matrices are great ways to review your model performance for a multi-class classification problem. Being able to identify which class the misclassified observations end up in is a great way to determine if you need to build additional features to improve your overall model. In the example below we plot a regular counts confusion matrix as well as a weighted percent confusion matrix. The percent confusion matrix is particulary helpful when you have unbalanced class sizes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "wSWGVZU6SpOW"
   },
   "outputs": [],
   "source": [
    "class_names=['isolated','released','missing','deceased'] # name  of classes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "GjnV5ugJSpOb"
   },
   "outputs": [],
   "source": [
    "import itertools\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from sklearn import svm, datasets\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import confusion_matrix\n",
    "\n",
    "def plot_confusion_matrix(cm, classes,\n",
    "                          normalize=False,\n",
    "                          title='Confusion matrix',\n",
    "                          cmap=plt.cm.Blues):\n",
    "    \"\"\"\n",
    "    This function prints and plots the confusion matrix.\n",
    "    Normalization can be applied by setting `normalize=True`.\n",
    "    \"\"\"\n",
    "    if normalize:\n",
    "        cm = cm.astype('float') / cm.sum(axis=1)[:, np.newaxis]\n",
    "        print(\"Normalized confusion matrix\")\n",
    "    else:\n",
    "        print('Confusion matrix, without normalization')\n",
    "\n",
    "    print(cm)\n",
    "\n",
    "    plt.imshow(cm, interpolation='nearest', cmap=cmap)\n",
    "    plt.title(title)\n",
    "    plt.colorbar()\n",
    "    tick_marks = np.arange(len(classes))\n",
    "    plt.xticks(tick_marks, classes, rotation=45)\n",
    "    plt.yticks(tick_marks, classes)\n",
    "\n",
    "    fmt = '.2f' if normalize else 'd'\n",
    "    thresh = cm.max() / 2.\n",
    "    for i, j in itertools.product(range(cm.shape[0]), range(cm.shape[1])):\n",
    "        plt.text(j, i, format(cm[i, j], fmt),\n",
    "                 horizontalalignment=\"center\",\n",
    "                 color=\"white\" if cm[i, j] > thresh else \"black\")\n",
    "\n",
    "    plt.ylabel('True label')\n",
    "    plt.xlabel('Predicted label')\n",
    "    plt.tight_layout()\n",
    "\n",
    "\n",
    "# Compute confusion matrix\n",
    "cnf_matrix = confusion_matrix(y_test, y_pred)\n",
    "np.set_printoptions(precision=2)\n",
    "\n",
    "# Plot non-normalized confusion matrix\n",
    "plt.figure()\n",
    "plot_confusion_matrix(cnf_matrix, classes=class_names,\n",
    "                      title='Confusion matrix, without normalization')\n",
    "#plt.savefig('figures/RF_cm_multi_class.png')\n",
    "\n",
    "# Plot normalized confusion matrix\n",
    "plt.figure()\n",
    "plot_confusion_matrix(cnf_matrix, classes=class_names, normalize=True,\n",
    "                      title='Normalized confusion matrix')\n",
    "#plt.savefig('figures/RF_cm_proportion_multi_class.png', bbox_inches=\"tight\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "Z7PtbV4LSpOc"
   },
   "source": [
    "### Plot feature importances\n",
    "The random forest algorithm can be used as a regression or classification model. In either case it tends to be a bit of a black box, where understanding what's happening under the hood can be difficult. Plotting the feature importances is one way that you can gain a perspective on which features are driving the model predictions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "T1xpGOCVSpOc"
   },
   "outputs": [],
   "source": [
    "feature_importance = clf.feature_importances_\n",
    "# make importances relative to max importance\n",
    "feature_importance = 100.0 * (feature_importance / feature_importance.max())[:30]\n",
    "sorted_idx = np.argsort(feature_importance)[:30]\n",
    "\n",
    "pos = np.arange(sorted_idx.shape[0]) + .5\n",
    "print(pos.size)\n",
    "sorted_idx.size\n",
    "plt.figure(figsize=(10,10))\n",
    "plt.barh(pos, feature_importance[sorted_idx], align='center')\n",
    "plt.yticks(pos, X.columns[sorted_idx])\n",
    "plt.xlabel('Relative Importance')\n",
    "plt.title('Variable Importance')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "Ys_mI5GsSpOe"
   },
   "source": [
    "The popularity of random forest is primarily due to how well it performs in a multitude of data situations. It tends to handle highly correlated features well, where as a linear regression model would not. In this case study we demonstrate the performance ability even with only a few features and almost all of them being highly correlated with each other.\n",
    "Random Forest is also used as an efficient way to investigate the importance of a set of features with a large data set. Consider random forest to be one of your first choices when building a decision tree, especially for multiclass classifications."
   ]
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [
    "FXGd_NbdB5kn"
   ],
   "name": "RandomForest_casestudy_covid19.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
