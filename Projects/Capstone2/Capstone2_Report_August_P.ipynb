{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " <h1><center>Predicting Asthma Diagnosis As A Screening Tool</center></h1>\n",
    " "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[<center>Link to GitHub Repository<center>](https://github.com/August-JF-Perez/AugustP_Springboard/tree/main/Projects/Capstone2) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Introduction"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- This project aims to build a predictive model in order to screen whether a patient would be diagnosed with asthma.\n",
    "- The real-world application would be for doctors to more easily determine which patients to focus on, for the more efficient allocation of resources in an already strained healthcare system.\n",
    "- The final model was trained with 26 features/variables for each patient that encompass categories of demographic details, lifestyle factors, environmental and allergy factors, medical history, clinical measurements, symptoms, and including diagnosis indicator.\n",
    "\n",
    "- The final goal is to build a classification model with a focus on maximizing the recall score (sensitivity) to decrease false negatives."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Source"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The dataset used in the models explored was \"Asthma Disease Dataset\" from Kaggle\n",
    "\n",
    "[Link to Kaggle Dataset](https://www.kaggle.com/datasets/rabieelkharoua/asthma-disease-dataset?resource=download)\n",
    "\n",
    "Below is a snapshot of the raw data arranged in a dataframe after importing.\n",
    "\n",
    "![original_data_df](images/original_data_dataframe.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Cleaning & Preprocessing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The raw dataset has 2392 entries and 29 columns. \n",
    "\n",
    "To ensure clean data prior modeling, unnecessary features were removed (PatientID & DoctorInCharge) as they did not provide information useful for predicting diagnosis.\n",
    "\n",
    "Cleaning:\n",
    "- The data was checked for null values, outliers, value consistency within each feature, and detectable irregularities. No occurences requiring in-depth cleaning were found.\n",
    "\n",
    "Preprocessing:\n",
    "- Categorical features were confirmed to be or converted to indicators of 0 or 1 (dummy variable conversion)\n",
    "- Addition of additional features that would combine features within the same groups to give the magnitude of each feature group was included in the dataset with the goal of indicating that with compunding factors, the chance of asthma diagnosis would increase.\n",
    "- Standardize the magnitude of numeric features to have ranges from 0 to 1 (0 representing the minimum value of the feature, and 1 representing the maximum)\n",
    "- Resampling was performed to address the class imbalance of Non-Diagnosed vs Diagnosed for asthma\n",
    "    - Oversampling performed with replacement to achieve equal count of Diagnosis=0 & Diagnosis=1\n",
    "    - This was not performed on test/validation data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Below is a snapshot of the cleaned & preprocessed dataset.\n",
    "\n",
    "![clean_df](images/clean_dataframe.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# EDA: Hightlights"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**General Observations**\n",
    "\n",
    "- Continuous columns had relatively flat distributions for all data and when filtered for Diagnosis=0\n",
    "- Continuous columns had multiple different distributions when filtered for Diagnosis=1\n",
    "    - For each feature, there were peaks and valleys shown in the histogram but no peak was tall enough to make the value of the feature jump out as significant or shown to be deterministic for asthma\n",
    "- Categorical columns had very similar distributions when comparing Diagnosis= 0 and 1\n",
    "    - Suggesting that even if one class in the category was in majority, it did not serve as a good indicator by itself if the patient would get diagnosed with asthma"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Interactions Between Features** \n",
    "\n",
    "Target feature of the dataset: Diagnosis\n",
    "\n",
    "Correlation Heatmap:\n",
    "\n",
    "![corr_heatmap](images/corr_heatmap.png)\n",
    "\n",
    "\n",
    "- Correlation between two features\n",
    " - Highest\n",
    "     - 0.064841 between BMI & DustExposure\n",
    " - Lowest\n",
    "     - -0.059298 between Wheezing & Hayfever\n",
    "\n",
    "- Correlation between the target (diagnosis) & a feature\n",
    "    - Highest\n",
    "        - 0.053956 for ExerciseInduced\n",
    "    - Lowest\n",
    "        - -0.039278 for Chesttightness\n",
    "\n",
    "These correlation correficients indicate there is barely any relationship between features and a relationship between a single feature and Diagnosis. With a correlation of 1 or -1 being a perfect linear relationship."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"Figures/count.png\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Balanced vs Imbalanced Dataset**\n",
    "\n",
    "- The dataset suffered a class imbalance between Diagnosis=0 (Non-Diagnosed for asthma) & Diagnosis=1 (Diagnosed for asthma).\n",
    "- This implies that  two-class ML modeling may suffer from imbalance in the dataset.\n",
    "- Observations where Diagnosis=1 account for 5.18% of all the data.\n",
    "\n",
    "- This imbalance was addressed as noted in the Data Cleaning & Preprocessing section"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model Results"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Methodology**\n",
    "- Choose multiple classification models in the SciKit Learn library that could be reasonably applied to this two-class prediction problem\n",
    "- Fit on data that has been balanced between classes and test/validate on data that did not recieve artifical class balancing\n",
    "- Return scores and model metrics with a **focus on recall (for Diagnosis = 1)**\n",
    "- Choose a best 2 models and perform hyperparameter tuning\n",
    "- Choose best model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Baseline: Random Guessing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- The baseline for which to compare classification models to was created using SciKit Learn's DummyClassifier() function to **simulate random guessing**.\n",
    "    - Applying a Grid Search, the dummy classifier hyperparameter 'strategy' was set to the value 'uniform'\n",
    "- Continuiing the focus on maximizing the recall score, **randomly guessing gave a recall of 37.9%**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Preliminary Modeling"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- 6 classification models were trained and scored based on their predictions\n",
    "- Some values for model hyperparameters were changed based on applicability to the problem the model was applied to.\n",
    "- The 2 best models then underwent hyperparameter tuning"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Models and their classification reports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style>\n",
       "  table {margin-left: 0 !important;}\n",
       "</style>\n",
       "<!-- To left-align the classification report tables -->\n"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "%%html\n",
    "<style>\n",
    "  table {margin-left: 0 !important;}\n",
    "</style>\n",
    "<!-- To left-align the classification report tables -->"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model 1: Decision Tree\n",
    "\n",
    "Classification Report:\n",
    "\n",
    "|              |   precision |   recall |   f1-score |   support |\n",
    "|:-------------|------------:|---------:|-----------:|----------:|\n",
    "| 0            |        0.94 |     0.95 |       0.95 |     565   |\n",
    "| 1            |        0.03 |     0.03 |       0.03 |      33   |\n",
    "| accuracy     |        0.9  |     0.9  |       0.9  |       0.9 |\n",
    "| macro avg    |        0.49 |     0.49 |       0.49 |     598   |\n",
    "| weighted avg |        0.89 |     0.9  |       0.89 |     598   |\n",
    "\n",
    "\n",
    "Decision Tree:\n",
    "\n",
    "    - Accuracy: 91%\n",
    "    - Recall: 3%\n",
    "    - High scores for diagnosis=0, low scores for diagnosis=1\n",
    "    - Worse recall than dummy classifier\n",
    "\n",
    "### Model 2: Random Forest\n",
    "\n",
    "Classification Report:\n",
    "|              |   precision |   recall |   f1-score |   support |\n",
    "|:-------------|------------:|---------:|-----------:|----------:|\n",
    "| 0            |        0.94 |     0.95 |       0.95 |     565   |\n",
    "| 1            |        0.03 |     0.03 |       0.03 |      33   |\n",
    "| accuracy     |        0.9  |     0.9  |       0.9  |       0.9 |\n",
    "| macro avg    |        0.49 |     0.49 |       0.49 |     598   |\n",
    "| weighted avg |        0.89 |     0.9  |       0.89 |     598   |\n",
    "\n",
    "Random Forest:\n",
    "\n",
    "    - Accuracy: 94%\n",
    "    - Recall: 0%\n",
    "    - Worse recall than dummy classifier\n",
    "\n",
    "Performed well for diagnosis=0, extremely porrly for predicting diagnosis=1\n",
    "\n",
    "### Model 3: KNN\n",
    "\n",
    "Classification Report:\n",
    "|              |   precision |   recall |   f1-score |   support |\n",
    "|:-------------|------------:|---------:|-----------:|----------:|\n",
    "| 0            |        0.94 |     0.84 |       0.89 |     565   |\n",
    "| 1            |        0.04 |     0.12 |       0.06 |      33   |\n",
    "| accuracy     |        0.8  |     0.8  |       0.8  |       0.8 |\n",
    "| macro avg    |        0.49 |     0.48 |       0.48 |     598   |\n",
    "| weighted avg |        0.89 |     0.8  |       0.85 |     598   |\n",
    "\n",
    "KNN:\n",
    "\n",
    "    - Accuracy: 80%\n",
    "    - Recall (diag=1): 12%\n",
    "    - Worse recall than dummy classifier\n",
    "\n",
    "KNN did well predicting diagnosis=0, slightly better than previous models but still poorly in predicting diagnosis=1\n",
    "\n",
    "### Model 4: Logistic Regression\n",
    "\n",
    "Classification Report:\n",
    "|              |   precision |   recall |   f1-score |   support |\n",
    "|:-------------|------------:|---------:|-----------:|----------:|\n",
    "| 0            |        0.95 |     0.59 |       0.73 |    565    |\n",
    "| 1            |        0.06 |     0.45 |       0.11 |     33    |\n",
    "| accuracy     |        0.58 |     0.58 |       0.58 |      0.58 |\n",
    "| macro avg    |        0.5  |     0.52 |       0.42 |    598    |\n",
    "| weighted avg |        0.9  |     0.58 |       0.69 |    598    |\n",
    "\n",
    "Logistic Regression:\n",
    "\n",
    "    - Accuracy: 58%\n",
    "    - Recall: 45%\n",
    "    - Improved recall over dummy classifier\n",
    "\n",
    "Low accuracy but much higher recall vs other models.\n",
    "\n",
    "I theorize this is because the grouping of the data points makes it difficult to make a decision plane such that the model will get about half predictions correct\n",
    "\n",
    "\n",
    "### Model 5: Gradient Boosting\n",
    "    Using decision trees\n",
    "\n",
    "Classification Report:\n",
    "|              |   precision |   recall |   f1-score |   support |\n",
    "|:-------------|------------:|---------:|-----------:|----------:|\n",
    "| 0            |        0.94 |     0.92 |       0.93 |    565    |\n",
    "| 1            |        0.04 |     0.06 |       0.05 |     33    |\n",
    "| accuracy     |        0.88 |     0.88 |       0.88 |      0.88 |\n",
    "| macro avg    |        0.49 |     0.49 |       0.49 |    598    |\n",
    "| weighted avg |        0.89 |     0.88 |       0.89 |    598    |\n",
    "\n",
    "Gradient Boosting:\n",
    "\n",
    "    - Accuracy: 88%\n",
    "    - Recall: 6%\n",
    "    - Worse recall than dummy classifier\n",
    "\n",
    "Accuracy lower than all models except Logistic Regression (at 58%). Recall (sensitivity) is only better than the tree & forest models.\n",
    "\n",
    "\n",
    "### Model 6: AdaBoost classifier\n",
    "    Using Logistic Regression since that has given the best recall so far\n",
    "\n",
    "Classification Report:\n",
    "|              |   precision |   recall |   f1-score |   support |\n",
    "|:-------------|------------:|---------:|-----------:|----------:|\n",
    "| 0            |        0.95 |     0.59 |       0.73 |    565    |\n",
    "| 1            |        0.06 |     0.45 |       0.11 |     33    |\n",
    "| accuracy     |        0.58 |     0.58 |       0.58 |      0.58 |\n",
    "| macro avg    |        0.5  |     0.52 |       0.42 |    598    |\n",
    "| weighted avg |        0.9  |     0.58 |       0.69 |    598    |\n",
    "\n",
    "AdaBoost Classifier:\n",
    "\n",
    "    - Accuracy: 58%\n",
    "    - Recall: 45%\n",
    "\n",
    "Almost exact same results as Logistic Regression by itself. Changing weights does not seem to have an effect using default values"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Confusion Matrixes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![tree_confusion_matrix](images/tree_confusion_matrix.png)\n",
    "![forest_confusion_matrix](images/forest_confusion_matrix.png)\n",
    "![knn_confusion_matrix](images/knn_confusion_matrix.png)\n",
    "![logreg_confusion_matrix](images/logreg_confusion_matrix.png)\n",
    "![gboost_confusion_matrix](images/gboost_confusion_matrix.png)\n",
    "![ada_confusion_matrix](images/ada_confusion_matrix.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note that KNN, Logistic Regression, & AdaBoost(LogisticRegression) have the greatest amount of correctly predicted for Diagnosis=1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### The 3 models with the highest recall for Diagnosis=1 were KNN, Logistic Regression, & AdaBoost(LogisticRegression)\n",
    "\n",
    "- KNN recall: 12%\n",
    "- Logistic Regression recall: 45%\n",
    "- AdaBoost classifier recall: 45%"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Hyperparameter Tuning\n",
    "    Of the models: KNN, Logistic Regression, & AdaBoost(LogisticRegression)\n",
    "\n",
    "A hyperparamter optimization was performed by applying the GridSearch approach to find the best modeling parameters and further improve the prediction recall (for Diagnosis = 1) of the models. The results show marginally slight improvement in the performance for each classifier after hyperparamter tuning.\n",
    "\n",
    "\n",
    "Logistic Regression & AdaBoost(LogisticRegression) had the highest recall for Diagnosis = 1 (about 45% for each) and were chosen for hyperparameter tuning.\n",
    "\n",
    "KNN only had a recall of about 12%. This model underwent hyperparameter tuning as a means of becoming more familiar with the tools utilized in this project.\n",
    "    \n",
    "    The results of tuning KNN are not shown in this report as they do not improve upon the original model or result in metrics better than Logistic Regression or AdaBoost. This is noted within this report as an indicator for when reviewing the project Jupyter Notebook."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Tuning Logistic Regression"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Best parameters:\n",
    " - 'class_weight': 'balanced'\n",
    " - 'max_iter': 500\n",
    " - 'random_state': 9\n",
    " - 'solver': 'liblinear'\n",
    "\n",
    "Best recall score: 43%"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Tuning AdaBoostClassifier"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Best parameters:\n",
    "- 'estimator': LogisticRegression(\n",
    "    - class_weight='balanced'\n",
    "    - max_iter=500\n",
    "    - solver='liblinear')\n",
    "- 'learning_rate': 0.1\n",
    "\n",
    "Best recall score: 46%"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## AdaBoost saw a 1% improvement in recall while the tuned Logistic Regression model saw a decrease in recall versus the default model.\n",
    "\n",
    "AdaBoost Recall Scores:\n",
    "- Original model: 45%\n",
    "- Tuned model: 46%\n",
    "\n",
    "Logistic Regression Recall Scores:\n",
    "- Original model: 45%\n",
    "- Tuned model: 43%"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Further Improvement: Text Classification With Keras Deep Learning Model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The methodology followed for applying deep learning model is similar to the steps taken in this [google workshop] for Keras BoW (https://github.com/tensorflow/workshops/blob/master/extras/keras-bag-of-words/keras-bow-model.ipynb). The tf-idf data into was split in to training and test sets. Next, a tokenizer method from keras library was applied to count the unique words in the vocabulary for this dataset and assign each of those words to indices. A fit_on_texts() function was called to create a word index lookup of the vocabulary in the dataset. The vocabulary was limited to the top words by passing a num_words param to the tokenizer. Then a  texts_to_matrix method was used to process the training data and test data in a format  that can be passed to the keras deep learning model.  Then a deep learning model was built by specifyin Keras the shape of the input data, output data, and the type of each layer. Keras can then fit the model to the input data and evaluates prediction accuracy using the test data.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Comparison of performance between the deep learning model and Naive Bayes model on original data shows that deep learning model performs significantly better than the Naive Bayes Classifier when the problem is extended to classes >6. Both model shows similar decreasing trend in the accuracy, but the overall roc_auc score is better for the deep learning model. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"Figures/NB_DL.png\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- The results stated above highlight that the deep learning model is less affected than the linear classifiers by the class imbalance in the given data (since roc_auc is decent for number of classes >6). It is possible that the threshold number of observations that the DL model requires to adequately train for each class is much less than whats required by a linear classifier. This was further verified by studying the effect of  increasing number of classes on the performance of the DL classifier using oversampled data. The results show that the roc_auc or accuracy are similar for both original and resampled dataset when a DL model is applied. As a result, it can be concluded that within the window of this investigation (i.e. number of classes between 2 to 15), unlike linear classifiers, deep neural network modeling is not impacted by imbalance in the data."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"Figures/DL_DLOS.png\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Conclusion"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- The overall accuracy at classifying the charcaters using different ML models was fairly low. \n",
    "\n",
    "- TF-IDF was found to be the best word embedding method to be used with Linear Classifiers. All the linear classifiers (SVM, Naive Bayes, Logisitic Regression) exhibited similar performance with Naive Bayes topping the list chart in terms of accuracy.\n",
    "\n",
    "- Investigation of the classifiers performance for increasing number of classes in the mutli classification task shows that roc_auc is a more reliable choice compared to accuracy to better understand different classifier's performance.\n",
    "\n",
    "- While the overall ROC_AUC scores obtained for different approaches are not great for both the linear classifiers and deep learning model, obtaining overall roc_auc scores >0.5 under different circumstances suggest that machine learning can capture unique traits about movie or TV show characters from reading past transcripts. In terms of roc_auc score, both kinds of classifiers are robust to changes in the number of classes (varied from 2 classes upto 15 classes) in the multiclass classification task.   \n",
    "\n",
    "- While modeling with linear classifiers for this dataset, the performance of a classifier can be impacted by imbalance in the dataset, and can be overcome by applying an oversampling technique. \n",
    "\n",
    "- Investigation of roc_auc score of specific classes show that the classifier can distinguish lines from the leading actors fairly well. The roc_auc score is the highest for identifying RACHEL. All the supporting actors considered has slightly low roc_auc score compared to the leading actors resembling the imbalance in the distribution of the data.\n",
    "\n",
    "- The rank of different classes in terms of roc_auc score does not follow their rank in terms of number of lines in the dataset. (e.g. Phoebe has the least number of lines compared to other characaters, but the roc_auc score is better than Ross or Chandler when modeled using a one vs rest linear classifier on imbalanced dataset). This suggests that ML models can capture the inherent features perticular to these characters and model's ability  do not entirely depend on the number of observations of the character.\n",
    "\n",
    "- Comparison of performance between the deep learning model and Naive Bayes model on original data shows that deep learning model performs significantly better than the Naive Bayes Classifier when the problem is extended to classes >6.\n",
    "\n",
    "- Deep learning model performs better than the linear classifiers. \n",
    "\n",
    "- Applying deep learning model to both original and oversampled datasets shows that the roc_auc scores or accuracies are similar for both original and resampled dataset. This implies that within the window of this investigation (i.e. number of classes between 2 to 15), unlike linear classifiers, deep neural network modeling is not impacted by imbalance in the data.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Future Work"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- One possible explanation for word2vec to not work well is that the corpus size from this dataset is small and does not provide enough information to capture word relationship in the embedding space. This can be explored by training word2vec on a very large corpus (e.g. pretrained models from 100 billion words in google news)  to get a very good word vector before embedding the dataset and will be considered in the future extension of this project. \n",
    "\n",
    "- Comparison of class specific ROC_AUC scores before an after oversampling suggests that the random oversampling algorithm adjusts the imbalance in the majority classes, but not in the minority classes. Applying deep learning model to both original and oversampled datasets shows that the roc_auc scores or accuracies are similar for both original and resampled dataset. The similarity suggests that  , unlike linear classifiers, deep neural network modeling is more robust  to imbalance in the data. Therefore , more advanced deep larning models can be attempted for better prediction with the current dataset,"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
